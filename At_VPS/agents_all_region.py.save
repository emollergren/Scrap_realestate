import requests
from bs4 import BeautifulSoup
import json
import time
from typing import Dict, List
from datetime import datetime
import re
import random
import csv
import os

def extract_price(price_text: str) -> float:
    """Extract numeric price value from text like '$705k'"""
    if not price_text:
        return 0
    # Remove $ and convert k/m to actual numbers
    price_text = price_text.replace('$', '').strip()
    multiplier = 1
    if 'k' in price_text.lower():
        multiplier = 1000
        price_text = price_text.lower().replace('k', '')
    elif 'm' in price_text.lower():
        multiplier = 1000000
        price_text = price_text.lower().replace('m', '')
    try:
        return float(price_text) * multiplier
    except:
        return 0

def safe_get_text(element, default=''):
    """Safely get text from a BeautifulSoup element"""
    try:
        return element.text.strip() if element else default
    except:
        return default

def safe_get_int(text, default=0):
    """Safely convert text to integer"""
    try:
        if not text:
            return default
        # Extract first number found in text
        match = re.search(r'\d+', text)
        return int(match.group()) if match else default
    except:
        return default

def safe_get_float(text, default=0.0):
    """Safely convert text to float"""
    try:
        if not text:
            return default
        return float(text.strip())
    except:
        return default

def extract_agent_data(agent_card) -> Dict:
    """Extract data from a single agent card"""
    try:
        # Get the profile section
        profile_section = agent_card.find('div', class_='agent-card__profile')
        if not profile_section:
            print("Skipping agent card - missing profile section")
            return {}

        # Get agent URL and name
        agent_link = profile_section.find('a', href=lambda x: x and '/agent/' in x)
        agent_url = agent_link['href'] if agent_link else ''
        
        # Get agent profile details
        agent_details = profile_section.find('div', class_='agent-profile__agent-details')
        if not agent_details:
            print("Skipping agent card - missing agent details")
            return {}

        # Get agent name
        agent_name = safe_get_text(agent_details.find('div', class_='agent-profile__name'))

        # Get agency name
        agency_name = safe_get_text(agent_details.find('div', class_='agent-profile__agency'))

        # Get role
        role = safe_get_text(agent_details.find('div', class_='agent-profile__role'))

        # Get rating and reviews
        ratings_container = agent_details.find('div', class_='RatingsReviewsAggregate__RatingsReviewsAggregateContainer-sc-6n7u1i-0')
        rating = 0.0
        review_count = 0
        recent_reviews = 0
        review_timeframe = ''

        if ratings_container:
            # Get rating
            rating_elem = ratings_container.find('p', class_='RatingsReviewsAggregate__AggregatedReview-sc-6n7u1i-3')
            rating = safe_get_float(safe_get_text(rating_elem))

            # Get total reviews
            review_count_elem = ratings_container.find('p', class_='RatingsReviewsAggregate__TotalReviews-sc-6n7u1i-4')
            review_count = safe_get_int(safe_get_text(review_count_elem))

            # Get recent reviews
            timeframe_elem = ratings_container.find('p', class_='RatingsReviewsAggregate__RatingsReviewsTimeframe-sc-6n7u1i-5')
            if timeframe_elem:
                recent_reviews = safe_get_int(safe_get_text(timeframe_elem))
                review_timeframe = safe_get_text(timeframe_elem)

        # Get stats section
        stats_section = agent_card.find('div', class_='agent-card__stats--always-inline')
        properties_sold = 0
        median_price = 0
        days_advertised = 0
        all_suburbs_sold = 0

        if stats_section:
            # Get properties sold
            properties_elem = stats_section.find('div', class_='key-feature__value')
            properties_sold = safe_get_int(safe_get_text(properties_elem))

            # Get median price
            price_elem = stats_section.find('div', class_='agent_card__money')
            median_price = extract_price(safe_get_text(price_elem))

            # Get days advertised
            days_elem = stats_section.find('div', class_='key-feature__value', string=lambda x: x and x.strip().isdigit())
            days_advertised = safe_get_int(safe_get_text(days_elem))

            # Get all suburbs sold
            all_suburbs_elem = stats_section.find('div', class_='AgentCardStats__AllSuburbsStatsWrapper-ocqj95-3')
            if all_suburbs_elem:
                all_suburbs_sold = safe_get_int(safe_get_text(all_suburbs_elem.find('div', class_='key-feature__value')))

        # Get review text
        review_section = agent_card.find('div', class_='agent-card__review')
        review_text = ''
        if review_section:
            review_text_elem = review_section.find('p', class_='iGRVPQ')
            review_text = safe_get_text(review_text_elem)

        # Only return data if we have at least the agent name
        if not agent_name:
            print("Skipping agent card - missing agent name")
            return {}

        return {
            "agent_name": agent_name,
            "agent_url": agent_url,
            "agency_name": agency_name,
            "role": role,
            "rating": rating,
            "review_count": review_count,
            "recent_reviews": recent_reviews,
            "review_timeframe": review_timeframe,
            "properties_sold": properties_sold,
            "median_price": median_price,
            "days_advertised": days_advertised,
            "all_suburbs_sold": all_suburbs_sold,
            "review_text": review_text
        }
    except Exception as e:
        print(f"Error extracting agent data: {str(e)}")
        return {}

def get_agents_from_page(url: str, page: int, max_retries: int = 3) -> List[Dict]:
    """Get all agents from a specific page with retry mechanism"""
    headers = {
        "authority": "www.realestate.com.au",
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "accept-encoding": "gzip, deflate, br, zstd",
        "accept-language": "en-US,en;q=0.9",
        "cache-control": "max-age=0",
        "cookie": "Country=DE; ew_bkt=73; reauid=24142017fd6b310009553f680b00000053210000; split_audience=e; AMCVS_341225BE55BBF7E17F000101%40AdobeOrg=1; _lr_geo_location_state=HE; _lr_geo_location=DE; _gcl_au=1.1.569147979.1748981023; s_ecid=MCMID%7C28492635512625225263999861222495772143; s_cc=true; _ga=GA1.1.1989317176.1748981024; DM_SitId1464=1; DM_SitId1464SecId12708=1; s_fid=0CA9FF112CCBF649-0AAE0BBF451175E3; s_vi=[CS]v1|341FAAA0D8A60ECE-400007EF38779535[CE]; tracking_acknowledged=true; DM_SitId1464SecId12707=1; AMCV_341225BE55BBF7E17F000101%40AdobeOrg=179643557%7CMCIDTS%7C20243%7CMCMID%7C28492635512625225263999861222495772143%7CMCAAMLH-1749626604%7C3%7CMCAAMB-1749626604%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1749029004s%7CNONE%7CMCAID%7CNONE%7CMCSYNCSOP%7C411-20250%7CvVersion%7C5.5.0; KFC=sjdK0abRYZNcQ25ouTCVkwv8cD4MB5tspeXVLtPleyY=|1749022023160; KP_UIDz-ssn=0bVt1ymTDm8QzQ5uikBwRsHmxV4aGy8WRFOHgmrrg7bPdvppgC8IL3nDl3EePHzyOElhKKrI1ufRLkO88rqbO8a09CG9aJjSK7cScgwTNsLtrhj363TffbQAjzllNy2D7iIwUqDoqT9lHwGFmifkNHkpzc1vpHiFeURaw4BE838; KP_UIDz=0bVt1ymTDm8QzQ5uikBwRsHmxV4aGy8WRFOHgmrrg7bPdvppgC8IL3nDl3EePHzyOElhKKrI1ufRLkO88rqbO8a09CG9aJjSK7cScgwTNsLtrhj363TffbQAjzllNy2D7iIwUqDoqT9lHwGFmifkNHkpzc1vpHiFeURaw4BE838; QSI_HistorySession=https%3A%2F%2Fwww.realestate.com.au%2Fagent%2Fjason-song-3220188%3FcampaignType%3Dinternal%26campaignChannel%3Din_product%26campaignSource%3Drea%26campaignName%3Dsell_enq%26campaignPlacement%3Dagent_search_result_card%26campaignKeyword%3Dagency_marketplace%26sourcePage%3Dagent_srp%26sourceElement%3Dagent_search_result_card~1749022834084%7Chttps%3A%2F%2Fwww.realestate.com.au%2Ffind-agent%2Falbany---greater-region-wa%3Fsource%3Dresults%26page%3D2~1749023132021; legs_sq=%5B%5BB%5D%5D; s_sq=%5B%5BB%5D%5D; KP2_UIDz-ssn=0a6PRO0W7rv3Idd6zdmsHVeq6eFsMgn271BwKRZSrWgOIZ2b8WIOlfpiiVT2FD2kNrUOVzdvcxpVa8vqQhIYA7hyIaSu9Mch93X9ZPs5c7EYxsgxGjJBJvr0ZkHwlP3e3i3jHSCYHPumTgfOHye5sQLjVYHPf7WvGXwA35vsRKl; KP2_UIDz=0a6PRO0W7rv3Idd6zdmsHVeq6eFsMgn271BwKRZSrWgOIZ2b8WIOlfpiiVT2FD2kNrUOVzdvcxpVa8vqQhIYA7hyIaSu9Mch93X9ZPs5c7EYxsgxGjJBJvr0ZkHwlP3e3i3jHSCYHPumTgfOHye5sQLjVYHPf7WvGXwA35vsRKl; pageview_counter.srs=26; utag_main=v_id:01973764651900126e899986ffeb0506f00280670086e$_sn:3$_se:17$_ss:0$_st:1749025073711$vapi_domain:realestate.com.au$_prevpage:rea%3Afind%20agent%3Aagent%3Asearch%20results%3Bexp-1749026874863$ses_id:1749021229742%3Bexp-session$_pn:14%3Bexp-session; s_nr30=1749023274864-Repeat; _ga_3J0XCBB972=GS2.1.s1749021225$o3$g1$t1749023275$j24$l0$h0; nol_fpid=84rvsky3kvta172cb5kqvd6rnrynu1748981024|1748981024769|1749023275258|1749023277007",
        "if-none-match": 'W/"e14b5-LQx/dP3xXO2cWx5XS1kvgiP20VE"',
        "priority": "u=0, i",
        "sec-ch-ua": '"Chromium";v="136", "Google Chrome";v="136", "Not.A/Brand";v="99"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "document",
        "sec-fetch-mode": "navigate",
        "sec-fetch-site": "none",
        "sec-fetch-user": "?1",
        "upgrade-insecure-requests": "1",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36"
    }

    for attempt in range(max_retries):
        try:
            # Add small delay before each request
            delay = random.uniform(2, 3)
            print(f"Waiting {delay:.1f} seconds before request...")
            time.sleep(delay)

            response = requests.get(url, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            agent_cards = soup.find_all('div', class_='agent-card')
            agents = []
            
            for i, card in enumerate(agent_cards, 1):
                print(f"\nProcessing agent {i} of {len(agent_cards)} on page {page}...")
                agent_data = extract_agent_data(card)
                if agent_data:
                    agents.append(agent_data)
                    print(f"Found agent: {agent_data['agent_name']} from {agent_data['agency_name']}")
            
            return agents

        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:  # Too Many Requests
                wait_time = (attempt + 1) * 30  # Exponential backoff: 30s, 60s, 90s
                print(f"Rate limited. Waiting {wait_time} seconds before retry...")
                time.sleep(wait_time)
                continue
            else:
                print(f"HTTP Error: {str(e)}")
                return []
        except Exception as e:
            print(f"Error fetching page {page}: {str(e)}")
            return []

    print(f"Failed to fetch page {page} after {max_retries} attempts")
    return []

def create_region_url(region_name: str, state: str) -> str:
    """Create URL for a region"""
    # Convert region name to URL format
    region_url = region_name.lower().replace(' - ', '---').replace(' ', '-')
    return f"https://www.realestate.com.au/find-agent/{region_url}-{state.lower()}"

def save_to_csv(agents_data: List[Dict], filename: str):
    """Save agents data to CSV file"""
    try:
        # Define CSV headers
        headers = [
            'agent_name', 'agent_url', 'agency_name', 'role', 'rating', 
            'review_count', 'recent_reviews', 'review_timeframe', 
            'properties_sold', 'median_price', 'days_advertised', 
            'all_suburbs_sold', 'review_text', 'region_name', 'region_state', 
            'region_id'
        ]
        
        # Write to CSV
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            writer.writeheader()
            writer.writerows(agents_data)
        print(f"\nData saved to {filename}")
    except Exception as e:
        print(f"Error saving to CSV file: {str(e)}")

def main():
    # Load regions from JSON file
    try:
        with open('regions.json', 'r', encoding='utf-8') as f:
            regions = json.load(f)
    except Exception as e:
        print(f"Error loading regions.json: {str(e)}")
        return

    # Initialize output files
    json_output = "Agents.json"
    csv_output = "Agents.csv"
    
    # Initialize data structures
    regions_data = {}  # For JSON: region-centric structure
    all_agents = []    # For CSV: flat agent list
    
    # Load existing data if file exists
    try:
        with open(json_output, 'r', encoding='utf-8') as f:
            regions_data = {region['id']: region for region in json.load(f)}
            # Convert existing data to flat list for CSV
            for region in regions_data.values():
                all_agents.extend(region['agents'])
            print(f"Loaded existing data from {json_output}")
    except FileNotFoundError:
        print(f"Starting new files: {json_output} and {csv_output}")
        # Initialize regions_data with empty agents list for each region
        for region in regions:
            region_id = region['id']
            regions_data[region_id] = {
                "name": region['name'],
                "state": region['state'],
                "id": region_id,
                "agents": []
            }
    except Exception as e:
        print(f"Error loading existing file: {str(e)}")
        # Initialize regions_data with empty agents list for each region
        for region in regions:
            region_id = region['id']
            regions_data[region_id] = {
                "name": region['name'],
                "state": region['state'],
                "id": region_id,
                "agents": []
            }

    # Process each region
    for region in regions:
        region_name = region['name']
        state = region['state']
        region_id = region['id']
        
        print(f"\nProcessing region: {region_name}, {state}")
        
        # Create URL for the region
        base_url = create_region_url(region_name, state)
        page = 1
        
        while True:
            url = f"{base_url}?source=results&page={page}"
            print(f"\nFetching page {page} for {region_name}...")
            
            agents = get_agents_from_page(url, page)
            if not agents:
                break
                
            # Add agents to both data structures
            regions_data[region_id]['agents'].extend(agents)  # For JSON
            
            # Add region information to agents for CSV
            for agent in agents:
                agent_with_region = agent.copy()
                agent_with_region['region_name'] = region_name
                agent_with_region['region_state'] = state
                agent_with_region['region_id'] = region_id
                all_agents.append(agent_with_region)
            
            # Save data every 4 pages
            if page % 4 == 0:
                try:
                    # Save to JSON (region-centric structure)
                    with open(json_output, 'w', encoding='utf-8') as f:
                        json.dump(list(regions_data.values()), f, indent=4, ensure_ascii=False)
                    
                    # Save to CSV (flat agent list)
                    save_to_csv(all_agents, csv_output)
                    
                    print(f"\nSaved data to both {json_output} and {csv_output}")
                    print(f"- JSON: {len(regions_data)} regions with their agents")
                    print(f"- CSV: {len(all_agents)} total agents")
                    
                    # Take a break after saving
                    delay = random.uniform(15, 20)
                    print(f"Taking a {delay:.1f} second break...")
                    time.sleep(delay)
                except Exception as e:
                    print(f"Error saving data: {str(e)}")
            
            page += 1
        
        # Take a longer break between regions
        delay = random.uniform(10, 20)
        print(f"\nCompleted region {region_name}. Taking a {delay:.1f} second break before next region...")
        time.sleep(delay)
    
    # Final save
    try:
        # Save to JSON (region-centric structure)
        with open(json_output, 'w', encoding='utf-8') as f:
            json.dump(list(regions_data.values()), f, indent=4, ensure_ascii=False)
        
        # Save to CSV (flat agent list)
        save_to_csv(all_agents, csv_output)
        
        print(f"\nFinal save completed:")
        print(f"- JSON: {len(regions_data)} regions with their agents")
        print(f"- CSV: {len(all_agents)} total agents")
    except Exception as e:
        print(f"Error saving final data: {str(e)}")

if __name__ == "__main__":
    main()
